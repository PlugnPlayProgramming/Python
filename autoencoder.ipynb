{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "autoencoder.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPK9jsm9+jCyJPGP4pqP+mp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PlugnPlayProgramming/python/blob/main/autoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Anomaly Detection by Auto Encoder"
      ],
      "metadata": {
        "id": "6ma2OO3vz82h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 입출력 폴더 생성\n",
        "!mkdir data_input\n",
        "!mkdir data_output\n",
        "\n",
        "# 원본 데이터 파일 다운로드\n",
        "!wget -O ./data_input/test_data.csv https://raw.githubusercontent.com/PlugnPlayProgramming/python/main/test_data.csv\n",
        "!wget -O ./data_input/train_data.csv https://raw.githubusercontent.com/PlugnPlayProgramming/python/main/train_data.csv\n",
        "!wget -O ./data_input/train_data_id.csv https://raw.githubusercontent.com/PlugnPlayProgramming/python/main/train_data_id.csv"
      ],
      "metadata": {
        "id": "Ne9OOSD74N2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YDfPWtQp0aVo"
      },
      "outputs": [],
      "source": [
        "# 모듈 및 전역변수\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "TIME_STEPS = 60     # 센서 데이터의 timestamp 개수\n",
        "fig_index = 1       # 차트파일 저장 색인\n",
        "csv_index = 1       # CSV파일 저장 색인"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. 공통 함수**\n",
        "*   시각화 그래프를 파일로 저장\n",
        "*   결과를 csv 파일로 저장\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YlG7oeE_2UoP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 시각화 그래프를 파일로 저장\n",
        "def my_plot(plt, bigsize=False) :\n",
        "    global fig_index\n",
        "\n",
        "    fig = plt.gcf()\n",
        "    if not bigsize :\n",
        "        fig.set_size_inches(8, 6)    # 크기(inch 단위)  640*480(in 80dpi)\n",
        "    else :\n",
        "        fig.set_size_inches(20, 15)  # 크기(inch 단위) 1600*1200(in 80dpi)\n",
        "\n",
        "    plt.savefig(\"./data_output/Figure_\" + str(fig_index).zfill(2) + \".png\")\n",
        "    fig_index += 1\n",
        "    # plt.show()\n",
        "\n",
        "    plt.close()\n",
        "\n",
        "# 결과를 csv 파일로 저장\n",
        "def my_export(np, fname, score):\n",
        "    global csv_index\n",
        "    np.savetxt(\"./data_output/Paper_\" + str(csv_index).zfill(2) + \"(\" + fname + \").csv\",\n",
        "               score, delimiter=\",\", fmt=\"%s\")\n",
        "    csv_index += 1"
      ],
      "metadata": {
        "id": "mEF1TE_90H81"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. 센싱 데이터 로딩(센서 1개 1년치)**\n",
        "* 테스트 데이터 : 7건\n",
        "* 훈련 데이터  : 28,678건\n",
        "* 훈련 데이터의 레이블(순번, 일자, 시각)"
      ],
      "metadata": {
        "id": "B6DX4vIu20_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('./data_input/test_data.csv')\n",
        "test_sensor = np.expand_dims(df.to_numpy(), axis=2)\n",
        "\n",
        "df = pd.read_csv('./data_input/train_data.csv')\n",
        "train_sensor = np.expand_dims(df.to_numpy(), axis=2)\n",
        "\n",
        "df = pd.read_csv('./data_input/train_data_id.csv', header=None)\n",
        "train_file_id = df[0].values.tolist()"
      ],
      "metadata": {
        "id": "wdXpMHMB0P-O"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. 센싱 데이터 정규화 및 모델생성**\n"
      ],
      "metadata": {
        "id": "wHtjlyRh3Dos"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 정규화 standard scalar= (x-mean)/std\n",
        "training_mean = train_sensor.mean()\n",
        "training_std = train_sensor.std()\n",
        "df_training_value = (train_sensor - training_mean) / training_std\n",
        "print(\"Number of training samples:\", len(df_training_value))\n",
        "\n",
        "x_train = df_training_value"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKtEug8i0RzD",
        "outputId": "580c2b56-db03-426b-c1ad-a64342a9a5e4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training samples: 28678\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1D CNN기반 Autoencoder Model 생성\n",
        "# Dropout Layer를 추가하여 Overfitting 방지\n",
        "print(\"Input data shape:\", np.shape(x_train))\n",
        "model = keras.Sequential([\n",
        "        layers.Input(shape=(x_train.shape[1], x_train.shape[2])),\n",
        "        layers.Conv1D(64, 7, 1, \"same\", activation=\"relu\"),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.Conv1D(32, 7, 1, \"same\", activation=\"relu\"),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.Conv1D(16, 7, 1, \"same\", activation=\"relu\"),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.Conv1D(8, 7,  1, \"same\", activation=\"relu\"),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.Conv1D(4, 7,  1, \"same\", activation=\"relu\"),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.Conv1D(2, 7,  1, \"same\", activation=\"relu\"),\n",
        "        layers.Conv1DTranspose(2, 7, 1, \"same\", activation=\"relu\"),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.Conv1DTranspose(4, 7, 1, \"same\", activation=\"relu\"),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.Conv1DTranspose(8, 7, 1, \"same\", activation=\"relu\"),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.Conv1DTranspose(16, 7, 1, \"same\", activation=\"relu\"),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.Conv1DTranspose(32, 7, 1, \"same\", activation=\"relu\"),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.Conv1DTranspose(64, 7, 1, \"same\", activation=\"relu\"),\n",
        "        layers.Conv1DTranspose(1, 7, 1, \"same\"),\n",
        "    ])\n",
        "\n",
        "#  - Adam optimizer 사용, 0.001 learning rate 설정\n",
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss=\"mse\")\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3ksb-2A0VIl",
        "outputId": "100cc952-9979-42fb-cd80-c4ab339abb40"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input data shape: (28678, 60, 1)\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d (Conv1D)             (None, 60, 64)            512       \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 60, 64)            0         \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 60, 32)            14368     \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 60, 32)            0         \n",
            "                                                                 \n",
            " conv1d_2 (Conv1D)           (None, 60, 16)            3600      \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 60, 16)            0         \n",
            "                                                                 \n",
            " conv1d_3 (Conv1D)           (None, 60, 8)             904       \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 60, 8)             0         \n",
            "                                                                 \n",
            " conv1d_4 (Conv1D)           (None, 60, 4)             228       \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 60, 4)             0         \n",
            "                                                                 \n",
            " conv1d_5 (Conv1D)           (None, 60, 2)             58        \n",
            "                                                                 \n",
            " conv1d_transpose (Conv1DTra  (None, 60, 2)            30        \n",
            " nspose)                                                         \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 60, 2)             0         \n",
            "                                                                 \n",
            " conv1d_transpose_1 (Conv1DT  (None, 60, 4)            60        \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 60, 4)             0         \n",
            "                                                                 \n",
            " conv1d_transpose_2 (Conv1DT  (None, 60, 8)            232       \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 60, 8)             0         \n",
            "                                                                 \n",
            " conv1d_transpose_3 (Conv1DT  (None, 60, 16)           912       \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 60, 16)            0         \n",
            "                                                                 \n",
            " conv1d_transpose_4 (Conv1DT  (None, 60, 32)           3616      \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " dropout_9 (Dropout)         (None, 60, 32)            0         \n",
            "                                                                 \n",
            " conv1d_transpose_5 (Conv1DT  (None, 60, 64)           14400     \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " conv1d_transpose_6 (Conv1DT  (None, 60, 1)            449       \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 39,369\n",
            "Trainable params: 39,369\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. 학습 실행**\n",
        "* Epoch=50, batch_size=128, validation=0.2%, early_stopping은 5로 설정하여 학습\n",
        "* Input과 Target을 동일하게 x_train으로 설정\n",
        "* Autoencoder는 reconsturction model로 Input과 Output의 차이가 작은 방향으로 학습함\n",
        "\n"
      ],
      "metadata": {
        "id": "-gLymCEM3X6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    x_train,\n",
        "    x_train,\n",
        "    epochs=50,\n",
        "    batch_size=128,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, mode=\"min\")],\n",
        ")\n",
        "\n",
        "# 학습된 모델 저장\n",
        "model.save(\"./data_output/sensor_model.h5\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-lWlqrg0V3r",
        "outputId": "76eb1160-6264-4167-cbeb-e1de7455c973"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "180/180 [==============================] - 16s 14ms/step - loss: 0.1635 - val_loss: 0.0807\n",
            "Epoch 2/50\n",
            "180/180 [==============================] - 1s 8ms/step - loss: 0.0293 - val_loss: 0.0346\n",
            "Epoch 3/50\n",
            "180/180 [==============================] - 1s 8ms/step - loss: 0.0179 - val_loss: 0.0191\n",
            "Epoch 4/50\n",
            "180/180 [==============================] - 1s 8ms/step - loss: 0.0113 - val_loss: 0.0046\n",
            "Epoch 5/50\n",
            "180/180 [==============================] - 1s 8ms/step - loss: 0.0074 - val_loss: 0.0054\n",
            "Epoch 6/50\n",
            "180/180 [==============================] - 1s 8ms/step - loss: 0.0056 - val_loss: 0.0031\n",
            "Epoch 7/50\n",
            "180/180 [==============================] - 1s 8ms/step - loss: 0.0044 - val_loss: 0.0038\n",
            "Epoch 8/50\n",
            "180/180 [==============================] - 1s 8ms/step - loss: 0.0037 - val_loss: 0.0019\n",
            "Epoch 9/50\n",
            "180/180 [==============================] - 1s 8ms/step - loss: 0.0033 - val_loss: 0.0035\n",
            "Epoch 10/50\n",
            "180/180 [==============================] - 1s 8ms/step - loss: 0.0028 - val_loss: 0.0026\n",
            "Epoch 11/50\n",
            "180/180 [==============================] - 1s 8ms/step - loss: 0.0025 - val_loss: 0.0019\n",
            "Epoch 12/50\n",
            "180/180 [==============================] - 1s 8ms/step - loss: 0.0022 - val_loss: 0.0031\n",
            "Epoch 13/50\n",
            "180/180 [==============================] - 1s 8ms/step - loss: 0.0020 - val_loss: 0.0019\n",
            "Epoch 14/50\n",
            "180/180 [==============================] - 1s 8ms/step - loss: 0.0018 - val_loss: 0.0047\n",
            "Epoch 15/50\n",
            "180/180 [==============================] - 1s 8ms/step - loss: 0.0018 - val_loss: 0.0024\n",
            "Epoch 16/50\n",
            "180/180 [==============================] - 1s 8ms/step - loss: 0.0016 - val_loss: 0.0024\n",
            "Epoch 17/50\n",
            "180/180 [==============================] - 1s 8ms/step - loss: 0.0015 - val_loss: 0.0015\n",
            "Epoch 18/50\n",
            "180/180 [==============================] - 1s 8ms/step - loss: 0.0014 - val_loss: 0.0018\n",
            "Epoch 19/50\n",
            "180/180 [==============================] - 1s 8ms/step - loss: 0.0014 - val_loss: 0.0026\n",
            "Epoch 20/50\n",
            "180/180 [==============================] - 1s 8ms/step - loss: 0.0013 - val_loss: 0.0022\n",
            "Epoch 21/50\n",
            "180/180 [==============================] - 1s 8ms/step - loss: 0.0012 - val_loss: 0.0015\n",
            "Epoch 22/50\n",
            "180/180 [==============================] - 1s 8ms/step - loss: 0.0013 - val_loss: 9.5174e-04\n",
            "Epoch 23/50\n",
            "180/180 [==============================] - 1s 8ms/step - loss: 9.7832e-04 - val_loss: 0.0014\n",
            "Epoch 24/50\n",
            "180/180 [==============================] - 1s 8ms/step - loss: 0.0012 - val_loss: 0.0015\n",
            "Epoch 25/50\n",
            "180/180 [==============================] - 1s 8ms/step - loss: 9.8029e-04 - val_loss: 0.0012\n",
            "Epoch 26/50\n",
            "180/180 [==============================] - 1s 8ms/step - loss: 9.1525e-04 - val_loss: 0.0013\n",
            "Epoch 27/50\n",
            "180/180 [==============================] - 1s 8ms/step - loss: 9.6150e-04 - val_loss: 0.0012\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. 학습결과 확인 및 임계치 산정**\n"
      ],
      "metadata": {
        "id": "lXi7NAQx3rFX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Epoch에 따른 loss 시각화 : 학습이 잘 되었음을 확인 가능\n",
        "plt.plot(history.history[\"loss\"], label=\"Training Loss\")\n",
        "plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Train & Validation Loss vs Epoch\")\n",
        "plt.legend()\n",
        "my_plot(plt)\n",
        "my_export(np, \"training_status_loss\", history.history[\"loss\"])\n",
        "my_export(np, \"training_status_val_loss\", history.history[\"val_loss\"])\n",
        "\n",
        "# 정규화 되어 있는 Input, Output을 복원하고, 그 차이의 mean을 구해, Anomaly Score 산출\n",
        "x_train_unno = (x_train * training_std) + training_mean\n",
        "x_train_pred = model.predict(x_train)\n",
        "x_train_pred_unno = (x_train_pred * training_std) + training_mean\n",
        "train_anomaly_score = np.mean(np.abs((x_train_pred_unno) - (x_train_unno)), axis=1)\n",
        "\n",
        "# Threshold를 추출하기 위해 학습데이터의 anomaly score를 histogram으로 표현,\n",
        "# anomaly score가 가장 빈번히 발생한 score영역을 탐색 (정상 데이터 군집)\n",
        "# peak기준 오른쪽 (anomaly score가 큰 영역에서 hist가 5보다 작은값을 Threhold로 선택)\n",
        "# 일반적으로는 학습데이터의 max anomaly score를 histogram으로 산정하나, 학습데이터에\n",
        "# 불량이 포함되어 있을 경우가 있음으로 hist가 5보다 작은곳을 Threhold로 선택함\n",
        "# 학습데이터가 1년치 데이터임으로, 1년동안 5번 이하로 발생한 센서 데이터이며 anomaly score가 클 경우를\n",
        "# Threshold로 산정했다는 의미임\n",
        "thresh = []\n",
        "hist, bins = np.histogram(train_anomaly_score, 60)\n",
        "hist = np.append(hist, np.array([0]))\n",
        "for i in range(0, len(hist)):\n",
        "    if hist[i] < 5:\n",
        "        thresh.append(bins[i])\n",
        "    if max(hist) == hist[i]:\n",
        "        peak_loc = i\n",
        "\n",
        "peak = bins[peak_loc]\n",
        "zero = 0\n",
        "thresh2 = []\n",
        "for i in range(0, len(thresh)):\n",
        "    if thresh[i] > peak:\n",
        "        thresh2.append(thresh[i])\n",
        "threshold = min(thresh2)\n",
        "\n",
        "plt.hist(train_anomaly_score, bins=60)\n",
        "plt.title(\"Trained Data - Predicted Data\")\n",
        "plt.xlabel(\"Train Anomaly Score\")\n",
        "plt.ylabel(\"No of samples\")\n",
        "my_plot(plt)\n",
        "my_export(np, \"train_anomaly_score\", train_anomaly_score)\n",
        "print(f\"Reconstruction threshold value: {threshold:3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LD8qUDEi0ggs",
        "outputId": "8b553995-0028-4972-c91d-30c3f1b47871"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reconstruction threshold value: 0.000138\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. 시각화**"
      ],
      "metadata": {
        "id": "8qv4_e9U3xyK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 모든 센서데이터를 동시에 PLOT하여 일반적으로 어떻게 sensor data가 움직였는지 확인\n",
        "for i in range(0, len(x_train_pred_unno)):\n",
        "    plt.plot(x_train_unno[i], label='Training Data')\n",
        "    plt.title(\"1 year Data\")\n",
        "    plt.xlabel(\"Time\")\n",
        "    plt.ylabel(\"Sensor Value\")\n",
        "my_plot(plt)\n",
        "one_year = np.squeeze(x_train_pred_unno)\n",
        "my_export(np, \"one_year\", one_year)\n",
        "\n",
        "# INPUT, OUTPUT을 PLOT하여 학습이 잘 되었는지 확인(5건)\n",
        "paper_train = x_train_unno[0]\n",
        "paper_predict = x_train_pred_unno[0]\n",
        "for i in range(0, 5):\n",
        "    plt.plot(x_train_unno[i], label='Ground')\n",
        "    plt.plot(x_train_pred_unno[i], label='Predicted')\n",
        "    plt.legend()\n",
        "    plt.title(\"Normal Data\")\n",
        "    plt.xlabel(\"Time\")\n",
        "    plt.ylabel(\"Sensor Value\")\n",
        "    my_plot(plt)\n",
        "    paper_train = np.hstack((paper_train, x_train_unno[i]))\n",
        "    paper_predict = np.hstack((paper_predict, x_train_pred_unno[i]))"
      ],
      "metadata": {
        "id": "LCuGovZ60hUQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. 불량 판정**"
      ],
      "metadata": {
        "id": "9_RTJTw032SM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# INPUT데이터를 사용하여 불량 판정하고 INPUT, OUPUT차이 확인\n",
        "anomalies = train_anomaly_score > threshold\n",
        "paper_train = np.delete(paper_train, 0, axis=1)\n",
        "paper_predict = np.delete(paper_predict, 0, axis=1)\n",
        "my_export(np, \"train\", paper_train)\n",
        "my_export(np, \"predict\", paper_predict)\n",
        "\n",
        "print(\"Train Data Anomaly Count\")\n",
        "print(\"Number of anomaly samples: \", np.sum(anomalies))\n",
        "\n",
        "alarm_list = []\n",
        "for i in range(0, len(x_train_pred_unno)):\n",
        "    if train_anomaly_score[i] > threshold:\n",
        "        alarm_list.append(train_file_id[i])\n",
        "\n",
        "with open(\"./data_output/Paper_99(alarm_list_step).csv\", \"w\") as f:\n",
        "    for i in range(len(alarm_list)):\n",
        "        f.write(alarm_list[i] + '\\n')\n",
        "\n",
        "paper_train = x_train_unno[0]\n",
        "paper_predict = x_train_pred_unno[0]\n",
        "k = 1\n",
        "for i in range(0, len(x_train_pred_unno)):\n",
        "    if train_anomaly_score[i] > threshold:\n",
        "        plt.subplot(5, 5, k)\n",
        "        plt.plot(x_train_unno[i], label='Ground')\n",
        "        plt.plot(x_train_pred_unno[i], label='Predicted')\n",
        "        plt.title(f\"{train_file_id[i]}\")\n",
        "        plt.xlabel(\"Time\")\n",
        "        plt.ylabel(\"Sensor Value\")\n",
        "        plt.legend()\n",
        "        k=k+1\n",
        "        paper_train = np.hstack((paper_train, x_train_unno[i]))\n",
        "        paper_predict = np.hstack((paper_predict, x_train_pred_unno[i]))\n",
        "        if k > 25:\n",
        "            plt.subplots_adjust(wspace=0.3, hspace=0.8)\n",
        "            my_plot(plt, bigsize=True)\n",
        "            k = 1\n",
        "\n",
        "plt.subplots_adjust(wspace=0.3, hspace=0.8)\n",
        "my_plot(plt, bigsize=True)\n",
        "\n",
        "paper_train = np.delete(paper_train, 0, axis=1)\n",
        "paper_predict = np.delete(paper_predict, 0, axis=1)\n",
        "my_export(np, \"train\", paper_train)\n",
        "my_export(np, \"predict\", paper_predict)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SrljU9n0jNh",
        "outputId": "96b7fe40-006a-4084-df78-7138f3dff470"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Data Anomaly Count\n",
            "Number of anomaly samples:  31\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9. 테스트 데이터를 통한 검증**"
      ],
      "metadata": {
        "id": "Ji3qwC0Z36s4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST 데이터 : 7개의 Sample, 불량 5개 정상 2개\n",
        "# TEST 데이터를 Train 데이터의 mean, std를 활용하여 정규화\n",
        "df_test_value = (test_sensor - training_mean) / training_std\n",
        "x_test = df_test_value\n",
        "x_test_unnor = df_test_value * training_std + training_mean\n",
        "\n",
        "#  - TEST데이터를 활용하여 예측, 정규화 했던 결과를 복원\n",
        "x_test_pred = model.predict(df_test_value)\n",
        "x_test_pred_unnor = x_test_pred * training_std + training_mean\n",
        "\n",
        "#  - TEST 데이터의 Anomaly Score 산출\n",
        "paper_test = x_test_unnor[0]\n",
        "paper_test_predict= x_test_pred_unnor[0]\n",
        "test_anomaly_score = np.mean(np.abs(x_test_pred_unnor - x_test_unnor), axis=1)\n",
        "test_anomaly_score = test_anomaly_score.reshape((-1))\n",
        "\n",
        "#  - TEST 데이터의 Anomaly 산출\n",
        "anomalies = test_anomaly_score > threshold\n",
        "paper_test = np.delete(paper_test, 0, axis=1)\n",
        "paper_test_predict = np.delete(paper_test_predict, 0, axis=1)\n",
        "my_export(np, \"test_abnormal\", paper_test)\n",
        "my_export(np, \"test_predict_abnormal\", paper_test_predict)\n",
        "\n",
        "#  - TEST 데이터의 Anomaly 표기\n",
        "for i in range(0, len(x_test)):\n",
        "    plt.plot(x_test_unnor[i], label='Ground')\n",
        "    plt.plot(x_test_pred_unnor[i], label='Predicted ')\n",
        "    if anomalies[i]==True:\n",
        "        plt.title(\"Test Data : Anomaly\", color='red', fontweight='bold')\n",
        "    else:\n",
        "        plt.title(\"Test Data : Normal\", color='green', fontweight='bold')\n",
        "    plt.xlabel(\"Time\")\n",
        "    plt.ylabel(\"Sensor Value\")\n",
        "    plt.legend()\n",
        "    my_plot(plt)\n",
        "    paper_test = np.hstack((paper_test, x_test_unnor[i]))\n",
        "    paper_test_predict = np.hstack((paper_test_predict, x_test_pred_unnor[i]))\n",
        "\n",
        "print(\"Test Data Anomaly Count\")\n",
        "print(\"Number of anomaly samples: \", np.sum(anomalies))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JiHVxxwh0lOO",
        "outputId": "3facd88f-f7d5-4c6f-e281-48048dbc098e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Data Anomaly Count\n",
            "Number of anomaly samples:  5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST 데이터의 Anomaly Score histogram plot\n",
        "# Threshold 기준 왼쪽에 데이터 2개 (정상), 오른쪽 데이터 5개 (불량) 존재 확인\n",
        "plt.hist(test_anomaly_score, bins=60)\n",
        "plt.title(\"TEST Anomaly Score\")\n",
        "plt.xlabel(\"Anomaly Score\")\n",
        "plt.ylabel(\"No of samples\")\n",
        "my_plot(plt)\n",
        "my_export(np, \"test_anomaly_score\", test_anomaly_score)"
      ],
      "metadata": {
        "id": "9t8TIwcq4AZa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}